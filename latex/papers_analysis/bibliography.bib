@misc{gan2021threedworld,
	title={The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI}, 
	author={Chuang Gan and Siyuan Zhou and Jeremy Schwartz and Seth Alter and Abhishek Bhandwaldar and Dan Gutfreund and Daniel L. K. Yamins and James J DiCarlo and Josh McDermott and Antonio Torralba and Joshua B. Tenenbaum},
	year={2021},
	eprint={2103.14025},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}
@INPROCEEDINGS{7942676,
	author={A. {Llopart} and O. {Ravn} and N. A. {Andersen}},
	booktitle={2017 3rd International Conference on Control, Automation and Robotics (ICCAR)}, 
	title={Door and cabinet recognition using Convolutional Neural Nets and real-time method fusion for handle detection and grasping}, 
	year={2017},
	volume={},
	number={},
	pages={144-149},
	doi={10.1109/ICCAR.2017.7942676}}

@INPROCEEDINGS{7380814,
	author={T. H. {Yuan} and F. H. {Hashim} and W. M. D. W. {Zaki} and A. B. {Huddin}},
	booktitle={2015 International Electronics Symposium (IES)}, 
	title={An automated 3D scanning algorithm using depth cameras for door detection}, 
	year={2015},
	volume={},
	number={},
	pages={58-61},
	doi={10.1109/ELECSYM.2015.7380814}}

@INPROCEEDINGS{9096155,
	author={J. G. {Ramôa} and L. A. {Alexandre} and S. {Mogo}},
	booktitle={2020 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)}, 
	title={Real-Time 3D Door Detection and Classification on a Low-Power Device}, 
	year={2020},
	volume={},
	number={},
	pages={96-101},
	doi={10.1109/ICARSC49921.2020.9096155}}

@INPROCEEDINGS{7743677,
	author={B. {Quintana} and S. A. {Prieto} and A. {Adán} and F. {Bosché}},
	booktitle={2016 International Conference on Indoor Positioning and Indoor Navigation (IPIN)}, 
	title={Door detection in 3D colored laser scans for autonomous indoor navigation}, 
	year={2016},
	volume={},
	number={},
	pages={1-8},
	doi={10.1109/IPIN.2016.7743677}}

@ARTICLE{7814251,

	author={R. {Ambruş} and S. {Claici} and A. {Wendt}},

	journal={IEEE Robotics and Automation Letters}, 

	title={Automatic Room Segmentation From Unstructured 3-D Data of Indoor Environments}, 

	year={2017},

	volume={2},

	number={2},

	pages={749-756},

	abstract={We present an automatic approach for the task of reconstructing a 2-D floor plan from unstructured point clouds of building interiors. Our approach emphasizes accurate and robust detection of building structural elements and, unlike previous approaches, does not require prior knowledge of scanning device poses. The reconstruction task is formulated as a multiclass labeling problem that we approach using energy minimization. We use intuitive priors to define the costs for the energy minimization problem and rely on accurate wall and opening detection algorithms to ensure robustness. We provide detailed experimental evaluation results, both qualitative and quantitative, against state-of-the-art methods and labeled ground-truth data.},

	keywords={image colour analysis;image reconstruction;indoor environment;minimisation;automatic room segmentation;unstructured 3D data;indoor environments;2D floor plan reconstruction;unstructured point clouds;building interiors;building structural elements;scanning device poses;reconstruction task;multiclass labeling problem;energy minimization;labeled ground-truth data;Three-dimensional displays;Two dimensional displays;Minimization;Semantics;Clutter;Image segmentation;Labeling;Mapping;RGB-D perception;semantic scene understanding},

	doi={10.1109/LRA.2017.2651939},

	ISSN={2377-3766},

	month={April},}

@article{QUINTANA2018146,
	title = "Door detection in 3D coloured point clouds of indoor environments",
	journal = "Automation in Construction",
	volume = "85",
	pages = "146 - 166",
	year = "2018",
	issn = "0926-5805",
	doi = "https://doi.org/10.1016/j.autcon.2017.10.016",
	url = "http://www.sciencedirect.com/science/article/pii/S0926580516302400",
	author = "B. Quintana and S.A. Prieto and A. Adán and F. Bosché",
	keywords = "Indoor spatial data model, 3D, Point cloud, Door detection, Building information model, Scan-to-BIM, Robot, Indoor navigation",
	abstract = "Door detection is becoming an increasingly important subject in building indoor modelling owing to its value in scan-to-BIM processes. This paper presents an original approach that detects open, semi-open and closed doors in 3D laser scanned data of indoor environments. The proposed technique is unique in that it integrates the information regarding both the geometry (i.e. XYZ coordinates) and colour (i.e. RGB or HSV) provided by a calibrated set of 3D laser scanner and a colour camera. In other words, our technique is developed in a 6D-space framework. The geometry-colour integration and other characteristics of our method make it robust to occlusion and variations in colours resulting from varying lighting conditions at each scanning location (e.g. specular highlights) and from different scanning locations. In addition to this paper, the authors also contribute a public dataset of real scenes along with an annotated ground truth. The dataset has varying levels of challenges and will help to assess the performance of new and existing contributions in the field. The approach proposed in this paper is tested against that dataset, yielding encouraging results."
}

@article{QUINTANA2016643,
	title = "Semantic scan planning for indoor structural elements of buildings",
	journal = "Advanced Engineering Informatics",
	volume = "30",
	number = "4",
	pages = "643 - 659",
	year = "2016",
	issn = "1474-0346",
	doi = "https://doi.org/10.1016/j.aei.2016.08.003",
	url = "http://www.sciencedirect.com/science/article/pii/S1474034616300453",
	author = "B. Quintana and S.A. Prieto and A. Adán and A.S. Vázquez",
	keywords = "Large-scale 3D scanning, Automatic 3D acquisition, Dense 3D reconstruction, Next best view in 3D environments, Large point cloud processing",
	abstract = "The objective of this paper is to propose a new semantic 3D data acquisition method which is focused on sensing data belonging to indoor structural elements of buildings. Our system uses and processes 3D information coming from a 3D laser scanner sensor. The presented approach deals with some essential key issues in the scanning world which are rarely dealt with in papers. These are: the final goal of the scanning process, the hypotheses about the scene, lack of dynamic spaces in the next-best-scan-based solutions and the quality evaluation of the data sensed. Whereas most of the Next Best Scan (NBS) based approaches do not discriminate between data and clutter, we propose a scanning process in which potential structural elements of building indoors are learned as a new scan arrives. Our workspace is not a priori hypothesized, but a dynamic space which is updated as a new scan is added. This allows us to deal with more complex shape scenarios (i.e. concave-shaped spaces). Through the so called Structural Element (SE) membership probability, we introduce the data-quality concept in the scanning process which highly reduces the point cloud to be processed. This system has been tested in inhabited indoors and has yielded promising results. An experimental comparison with three close techniques is presented in an extended and detailed experimental section. The results yielded from our experimental work demonstrate the quality and validity of the proposed method."
}

@INPROCEEDINGS{1467533,

	author={S. {Roth} and M. J. {Black}},

	booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 

	title={Fields of Experts: a framework for learning image priors}, 

	year={2005},

	volume={2},

	number={},

	pages={860-867 vol. 2},

	doi={10.1109/CVPR.2005.160}}

@INPROCEEDINGS{7090595,

	author={W. {Chen} and T. {Qu} and Y. {Zhou} and K. {Weng} and G. {Wang} and G. {Fu}},

	booktitle={2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)}, 

	title={Door recognition and deep learning algorithm for visual based robot navigation}, 

	year={2014},

	volume={},

	number={},

	pages={1793-1798},

	doi={10.1109/ROBIO.2014.7090595}}

@article{443235674,
	author = {Pinto, Nicolas and Cox, David and Dicarlo, James},
	year = {2008},
	month = {02},
	pages = {e27},
	title = {Why is Real-World Visual Object Recognition Hard?},
	volume = {4},
	journal = {PLoS computational biology},
	doi = {10.1371/journal.pcbi.0040027}
}

@article{D_az_Vilari_o_2015, title={3D Modeling of Building Indoor Spaces and Closed Doors from Imagery and Point Clouds}, volume={15}, ISSN={1424-8220}, url={http://dx.doi.org/10.3390/s150203491}, DOI={10.3390/s150203491}, number={2}, journal={Sensors}, publisher={MDPI AG}, author={Díaz-Vilariño, Lucía and Khoshelham, Kourosh and Martínez-Sánchez, Joaquín and Arias, Pedro}, year={2015}, month={Feb}, pages={3491–3512}}

@INPROCEEDINGS{8462922,

	author={M. {Brucker} and M. {Durner} and R. {Ambruş} and Z. C. {Márton} and A. {Wendt} and P. {Jensfelt} and K. O. {Arras} and R. {Triebel}},

	booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 

	title={Semantic Labeling of Indoor Environments from 3D RGB Maps}, 

	year={2018},

	volume={},

	number={},

	pages={1871-1878},

	doi={10.1109/ICRA.2018.8462922}}

@INPROCEEDINGS{8967568,

	author={K. {Zheng} and A. {Pronobis}},

	booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 

	title={From Pixels to Buildings: End-to-end Probabilistic Deep Networks for Large-scale Semantic Mapping}, 

	year={2019},

	volume={},

	number={},

	pages={3511-3518},

	doi={10.1109/IROS40897.2019.8967568}}

@ARTICLE{8683987,

	author={M. {Gao} and J. {Jiang} and G. {Zou} and V. {John} and Z. {Liu}},

	journal={IEEE Access}, 

	title={RGB-D-Based Object Recognition Using Multimodal Convolutional Neural Networks: A Survey}, 

	year={2019},

	volume={7},

	number={},

	pages={43110-43136},

	doi={10.1109/ACCESS.2019.2907071}}

@inproceedings{75b99ea2e5664bf3842099e1f5f7ca63,
	title = "MMSS: multi-modal sharable and specific feature learning for RGB-D object recognition",
	abstract = "Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.",
	author = "Anran Wang and Jianfei Cai and Jiwen Lu and Tat-Jen Cham",
	year = "2015",
	doi = "10.1109/ICCV.2015.134",
	language = "English",
	pages = "1125--1133",
	editor = "Ikeuchi, {Katsushi } and Christoph Schn{\"o}rr and Sivic, {Josef } and Ren{\'e} Vidal",
	booktitle = "Proceedings - 2015 IEEE International Conference on Computer Vision, ICCV 2015",
	publisher = "IEEE, Institute of Electrical and Electronics Engineers",
	address = "United States of America",
	note = "IEEE International Conference on Computer Vision 2015, ICCV 2015 ; Conference date: 07-12-2015 Through 13-12-2015",
	url = "https://ieeexplore.ieee.org/xpl/conhome/7407725/proceeding",
}

@article{47876543,
	author = {Socher, Richard and Huval, Brody and Bhat, Bharath and Manning, Christopher and Ng, Andrew},
	year = {2012},
	month = {01},
	pages = {},
	title = {Convolutional-Recursive Deep Learning for 3D Object Classification},
	volume = {1},
	journal = {NIPS}
}

@inproceedings{2342567,
	author = {Yunhua Yin and Huifang Li and Xinling Wen},
	title = {{Multi-model convolutional extreme learning machine with kernel for RGB-D object recognition}},
	volume = {10605},
	booktitle = {LIDAR Imaging Detection and Target Recognition 2017},
	editor = {Yueguang Lv and Weimin Bao and Weibiao Chen and Zelin Shi and Jianzhong Su and Jindong Fei and Wei Gong and Shensheng Han and Weiqi Jin and Jian Yang},
	organization = {International Society for Optics and Photonics},
	publisher = {SPIE},
	pages = {508 -- 517},
	keywords = {Object recognition, extreme learning machine, ELM, Convolutional neural network (CNN), RGB-D},
	year = {2017},
	doi = {10.1117/12.2292428},
	URL = {https://doi.org/10.1117/12.2292428}
}

@article{loghmani2019recurrent,
	author    = {Mohammad Reza Loghmani and
	Mirco Planamente and
	Barbara Caputo and
	Markus Vincze},
	title     = {Recurrent Convolutional Fusion for {RGB-D} Object Recognition},
	journal   = {CoRR},
	volume    = {abs/1806.01673},
	year      = {2018},
	url       = {http://arxiv.org/abs/1806.01673},
	archivePrefix = {arXiv},
	eprint    = {1806.01673},
	timestamp = {Mon, 13 Aug 2018 16:48:02 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1806-01673.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{8022892,

	author={U. {Asif} and M. {Bennamoun} and F. A. {Sohel}},

	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

	title={A Multi-Modal, Discriminative and Spatially Invariant CNN for RGB-D Object Labeling}, 

	year={2018},

	volume={40},

	number={9},

	pages={2051-2065},

	doi={10.1109/TPAMI.2017.2747134}}

@inproceedings{76543,
	author = {Cheng, Yanhua and Zhao, Xin and Huang, Kaiqi and Tan, T.},
	year = {2014},
	month = {07},
	pages = {},
	title = {Semi-supervised Learning for RGB-D Object Recogntion},
	doi = {10.13140/2.1.3208.2886}
}

@InProceedings{cheng2016semi-supervised,
	author = {Cheng, Yanhua and Zhao, Xin and Cai, Rui and Li (李志伟), Zhiwei and Huang, Kaiqi and Rui, Yong},
	title = {Semi-Supervised Multimodal Deep Learning for RGB-D Object Recognition},
	booktitle = {Proc. of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16)},
	year = {2016},
	month = {July},
	abstract = {This paper studies the problem of RGB-D object recognition. Inspired by the great success of deep convolutional neural networks (DCNN) in AI, researchers have tried to apply it to improve the performance of RGB-D object recognition. However, DCNN always requires a large-scale annotated dataset to supervise its training. Manually labeling such a large RGB-D dataset is expensive and time consuming, which prevents DCNN from quickly promoting this research area. To address this problem, we propose a semi-supervised multimodal deep learning framework to train DCNN effectively based on very limited labeled data and massive unlabeled data. The core of our framework is a novel diversity preserving co-training algorithm, which can successfully guide DCNN to learn from the unlabeled RGB-D data by making full use of the complementary cues of the RGB and depth data in object representation. Experiments on the benchmark RGB-D dataset demonstrate that, with only 5% labeled training data, our approach achieves competitive performance for object recognition compared with those state-of-the-art results reported by fully-supervised methods.},
	url = {https://www.microsoft.com/en-us/research/publication/semi-supervised-multimodal-deep-learning-rgb-d-object-recognition/},
	pages = {3345-3351},
	edition = {Proc. of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16)},
}

@article{yolov3,
	title={YOLOv3: An Incremental Improvement},
	author={Redmon, Joseph and Farhadi, Ali},
	journal = {arXiv},
	year={2018}
}

@article{DBLP:journals/corr/LinDGHHB16,
	author    = {Tsung{-}Yi Lin and
	Piotr Doll{\'{a}}r and
	Ross B. Girshick and
	Kaiming He and
	Bharath Hariharan and
	Serge J. Belongie},
	title     = {Feature Pyramid Networks for Object Detection},
	journal   = {CoRR},
	volume    = {abs/1612.03144},
	year      = {2016},
	url       = {http://arxiv.org/abs/1612.03144},
	archivePrefix = {arXiv},
	eprint    = {1612.03144},
	timestamp = {Mon, 13 Aug 2018 16:48:50 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/LinDGHHB16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
	
}

@INPROCEEDINGS{8594189,

	author={F. {Amigoni} and V. {Castelli} and M. {Luperto}},

	booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 

	title={Improving Repeatability of Experiments by Automatic Evaluation of SLAM Algorithms}, 

	year={2018},

	volume={},

	number={},

	pages={7237-7243},

	doi={10.1109/IROS.2018.8594189}}

@INPROCEEDINGS{613851,

	author={B. {Yamauchi}},

	booktitle={Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'}, 

	title={A frontier-based approach for autonomous exploration}, 

	year={1997},

	volume={},

	number={},

	pages={146-151},

	doi={10.1109/CIRA.1997.613851}}

@INPROCEEDINGS{7487796,

	author={N. {Sünderhauf} and F. {Dayoub} and S. {McMahon} and B. {Talbot} and R. {Schulz} and P. {Corke} and G. {Wyeth} and B. {Upcroft} and M. {Milford}},

	booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 

	title={Place categorization and semantic mapping on a mobile robot}, 

	year={2016},

	volume={},

	number={},

	pages={5729-5736},

	doi={10.1109/ICRA.2016.7487796}}

@article{alma991017170972206031,
	author = {B. Zhou},
	address = {Bethesda, MD : [Stanford, Calif.] :},
	issn = {0886-1714},
	journal = {News in physiological sciences.},
	lccn = {sn 98004861},
	publisher = {American Physiological Society},
	year = {2014},
	title = {Learning Deep Features for Scene Recognition using Places Database},
}

@ARTICLE {246776423,
	author = {},
	journal = {Computer},
	title = {PowerPC 601 and Alpha 21064: A Tale of Two RISCs},
	year = {1994},
	volume = {22},
	number = {06},
	issn = {1558-0814},
	pages = {46-58},
	keywords = {},
	doi = {10.1109/MC.1994.10056},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jun}
}