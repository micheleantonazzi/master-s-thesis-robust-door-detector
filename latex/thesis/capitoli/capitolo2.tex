\chapter{State of the art}
\label{capitolo2}
\thispagestyle{empty}

 Mobile robots are active agents that operate interacting with the real world. To successfully execute the assigned task, a mobile robot has to build an abstract model of the environment in which it operates. Considering indoor scenes, doors are crucial features that a robot can acquire to make its environment's model more informative. 
 Smart vacuum cleaners, healthcare robots or intelligent housekeepers helps people in their daily task. Usually, the tasks assigned to these autonomous agents imply moving between rooms and dealing with doors.
 Doors detection can help these types of agents to safely navigate in indoor environments, by improving their reasoning abilities and navigation strategies.
 
 \section{Doors Detection using Handcrafted Features}
  In literature, there are a lot of different studies concerning doors detection. One of the firsts attempts, described in \cite{sonarandivisualdoordetection}, combines visual information and sonar sensors to safely traverse doors by a B21 robot. Doors present a serious obstacle for this agent, so the goal consists of traversing opened doors using a certain angle to avoid collisions. This activity is divided into two sub-task: door detection and door crossing. The first is interesting for this thesis. \citeauthor{sonarandivisualdoordetection} consider an opened door as a squared noisy rectangular segment in an image. To detect it, this approach applies a vertical Sobel Filter to the grayscaled image. If there is a column wider than a certain threshold in the filtered image, it is considered a door. The sonar sensors are used to obtain the robot's distance from possible doors, to confirm the matches and avoid false positives. 
  
  The method in \cite{humanoid} proposes an integrated solution to detect a door and its knob by a humanoid robot. This solution recognizes the door's features with minor constraints and works sufficiently fast to performed online. \citeauthor{humanoid} argue that a door does not have very discriminative features, and the door's appearance can vary dramatically as viewpoint changes. To solve this issue, the proposed method classifies a frame using other reference images previously collected and analyzed. STAR Detector is used as a feature extractor and the features are classified using the Randomized Tree algorithm. At first, the reference images are parsed using the STAR Detector to find the typical characteristics of a door frame. Then, a Randomized Tree Classifier is trained using these features. At inference time, each new input image is processed by the STAR detector to extract features and, through the decision tree, each frame is classified. If a door is detected, the robot walks toward the door and detect its knob using a segmentation technique and some general constraints (size, ratio, and height bounds).
  
  Another method for recognizing doors in unfamiliar environments is described in \cite{edgeandcornerdoorsdetector}. In this work, doors are considered significant landmarks for navigation and self-localization not only for an autonomous agent but also for blind people. This work, proposed in \citeauthor{edgeandcornerdoorsdetector}, takes account of a variety of conditions, including differences in illumination, scale changes, deformation caused by perspective and occlusion, and variance of doors’ color, texture, or appearance. At first, images are converted in grayscale and smoothed by Gaussian lowpass filter. Then, edges and corners are extracted from the pre-processed frames, using respectively the Canny Edge Detector (\cite{canny}) and the method described in \cite{cornerdetector}. The authors then define a geometric shape model of doors, which is composed of two horizontal and two vertical lines between four corners. These features are then aggregated to find possible doors: only those groups that match the model are considered as true positives.  
 
 \section{Deep Learning in Object Detection}
 As the performance of hand-crafted features based detectors became saturated, Computer Vision researchers started to use deep learning methods to perform object detection. The evolution of this challenging problem in Computer Vision can be found in \cite{computervisionsurvey}. Today object detection strongly depends on the power of deep learning. 
 
 
 After the reborn of Convolutional Neural Networks (CNNs) in 2012, \citeauthor{rcnn} propose the first deep learning paradigm to detect objects, called RCNN (Region with CNN features). RCNNs, described in \cite{rcnn}, consists of three modules. The first generates category-independent region proposals, that are sub-portion of the same image. The second module is a large convolutional neural network (CNN) that extracts a fixed-length feature
 vector from each region. The third module is a set of class-specific linear SVMs, whose predict the presence of an object within each region proposal using the relative feature vectors. Despite the great progress brought by RCNN, its drawback is obvious: the detection speed is extremely slow (about 14s per
 image with GPU). This is caused by the redundant feature computation over a large number of overlapped region proposals (over 2000 per image). To overcome this limitation, \citeauthor{sppnet} propose Spatial Pyramid Pooling Networks (SPPNet). As described in \cite{sppnet}, this new network computes a single feature map of the entire image and then associates these features to the correspondent region proposal. This method avoids the repeatedly feature extraction phase from overlapped sub-portions of the same image.  Unlike the classic CNNs, SPPNet accepts as input images of arbitrary size and generates a fixed-length representation regardless of image size/scale. 
 
 In \cite{fastrcnn}, \citeauthor{fastrcnn} describes a new detector called Fast RCNN. This work unifies in the single end-to-end module the CNN responsible to extract features and the bounding box regressor, improving training and testing speed while also increasing accuracy. 
 
 The next step is to generate object proposals directly with a CNN model. This technique is explained in \cite{fasterrcnn}. \citeauthor{fasterrcnn} introduce Faster RCNN: the first near-realtime
 end-to-end deep learning detector. The main contribution of this work is the Region Proposal Network (RPN) that simultaneously predicts object bounds and objectness scores at each position. Since that RPN is a convolutional network, it can be trained jointly with the entire model by sharing convolutional layers in a unique end-to-end learning framework. The training procedure alternates between fine-tuning for the region proposal task and object detection phase, keeping the proposals fixed. 
 
 The methods described before are also defined ``two-stage detectors'', because they frame the detection as a ``coarse-to-fine'' process. In \cite{yolo}, \citeauthor{yolo} presented YOLO (You Only Look Once), introducing a new paradigm of deep models called ``one stage detectors''. This work doesn't follow the previous detection paradigm based on ``proposal detection + verification''. The neural network of YOLO is applied to the full image, performing the detection process in a single step. This method divides the image into a grid and predicts bounding boxes and class probabilities for each cell simultaneously. YOLO is also the first real-time detector: its enhanced version runs
 at 45fps while a lighter implementation reaches 155 fps (though with less detection quality).  Later, lots of improvements have been made around YOLO. \citeauthor{yolov2} proposed the v2 and v3 editions \cite{yolov2, yolov3}, improving both detection speed and accuracy. 
 
 Despite these great improvements, YOLO suffers from localization accuracy compared with two-stage detectors, especially for small objects. The second one-stage detector is called SSD (Single Shot MultiBox Detector), presented in \cite{ssd}. \citeauthor{ssd} introduce the multi-reference and multi-resolution detection techniques. The main idea is to define a set of anchor boxes with different scales and aspect radios at different locations of the same image, and then predict the bounding boxes and their class using these references. Using this method, SSD significantly improves the detection performance, also for small objects. 
 
 Despite their high speed, one-stage detectors are not yet able to overcome two-stage detectors in accuracy. \citeauthor{focalloss} discover the reason behind this fact: it consists of the extreme foreground-background class imbalance encountered during training. Following this intuition, the authors propose in \cite{focalloss} a new loss function, called Focal Loss, to put more focus on hard misclassified examples during training. Thanks to Focal Loss, the one-stage detectors achieve in accuracy the two-stage detectors, while maintaining very high detection speed. \citeauthor{focalloss} design a simple end-to-end module called RetinaNet to demonstrate the effectiveness of their proposed loss function. 
 
 More recently, \citeauthor{transformer} propose a new end-to-end paradigm called Transformer \cite{transformer}. Transformers demonstrate state-of-the-art results in Natural Language Process tasks, e.g. text classification, machine translation, and question answering. This new architecture intrigued researchers to study its application to computer vision problems. 
 
 In \cite{surveytransformer}, \citeauthor{surveytransformer} survey the history of Transformers' implication in Computer Vision. Transformers are based on encoder-decoder architecture with self-attention mechanism. In a sequence of items, self-attention technique estimates the relevance of each item to the others, capturing the interactions between them. This enables Transformers to model long dependencies between input sequence elements and support parallel processing. In \cite{detr}, \citeauthor{detr} present the first end-to-end model based on Transformer to perform object detection. DETR (DEtection TRansformer) extracts features from an image using a CNN backbone and then feeds them into a classic transformer to capture their relationships. Its outputs are post-processed by a linear regressor and a multilayer perceptron, whose infer the category labels and the bounding boxes, respectively.
 
 \section{Doors Detection with Deep Learning} 
 Deep learning methods outperform traditional approaches in Computer Vision. This is because classic methods based on handcrafted features are not generalizable, meaning the features (like edges or corners) needs to be specifically aggregated for different object categories. An end-to-end module, instead, learns automatically how to extract useful features that characterize an object. In particular, it extracts low-level features (e.g. corners or edges) in early stages and then aggregates them in a more complex manner. It is well known that these features are robust to scale, shift, rotation, and exposure changes. Given their advantages, end-to-end modules have become widely used in robotics, including for exploiting doors detection tasks. 
 
 The method proposed in \cite{detectdoorsfeature} is a vision-based technique for detecting doors by an autonomous agent. The main idea is to consider only color and shape information as useful features to detect doors in an office. This approach uses two neural classifiers to recognize these specific components in an image. One is trained for detecting the top, left, and the right bar of the door while the other is trained for detecting the door's corners. Then, a heuristic algorithm combines these features and check if they respect a typical door structure. A door is detected if at least three of these features are found in a frame and they respect the door's geometrical constraints. 
 
 Navigation is a challenging task for autonomous systems, especially in unknown and dynamic environments. The method described in \cite{doorsandnavigation} perform doors detection by an autonomous agent to improve its navigation strategy. This approach uses a convolutional neural network to detect doors in an indoor environment. For each door in the training dataset, \citeauthor{doorsandnavigation} collect five images taken from different locations. The final goal is give to the robot the rough location of doors, then make decisions of how to move to the doors and go through them.
 
 Another approach, described in \cite{doorcabinet}, focuses on robustly identify doors, cabinets, and their respective handles for robot grasping. \citeauthor{doorcabinet} uses a Convolutional Neural Network (based on YOLO) to detect the ROI (region of interest) of doors. Then, the proposed method obtains handle's point cloud using two different approaches. The first one is a visual segmentation approach based on k-means color c1usterization of the ROI while the former is a plane model extraction of the point cloud generated inside the region of interest. The ROI significantly reduces computational time and false-positive rates in the previous two phases.
 
 \section{The Limits of Deep Learning for Robotics}
 
 Due to the advantages and the excellent results of deep learning techniques, their application in robotics leads to specific problems and challenges that are not addressed by computer vision researchers. This is because a robot is an active agent that interacts with the real world and often operates in uncontrolled or detrimental conditions. Mistakes can lead to potentially catastrophic results and can even put human lives at risk, e.g. if the robot is a driverless car. In \cite{surveydeeplimits}, \citeauthor{surveydeeplimits} investigate the challenges of deep learning applied to robotics. They define the concept of robotic vision, which highly differs from computer vision. While the latter translates images into information, robotic vision translates images into actions, performed in the real world. For an autonomous agent, perception is only a small part of a more complicated and goal-driven system. The authors also survey the most most important learning challenges for a (deep) learning machine in a robotic vision context. The first is \textit{uncertainty estimation}. An autonomous agent has to estimate the uncertainty of its deep learning modules, considering them in the same way as other sensors. This is not trivial because deep entities return scores that are not calibrated probabilities, so their outputs are not usable in a Bayesian sensor fusion framework. In deep learning, it is assumed that a model operates in \textit{close-set} condition,  i.e. the classes encountered during deployment are known and exactly the same as during training. However, an autonomous agent operates in real-world environments, that are uncontrolled and extremely different from each other. In this context, a robot will encounter instances of classes, scenarios, textures, or environmental conditions that were not covered by the training data. On this \textit{open-set} conditions, it is crucial to identify the unknowns, not recognizing them as known classes. Other important challenges for robotics application using deep learning are \textit{incremental learning} and \textit{class-incremental learning}. The characteristics and appearance of objects may change a lot in the deployment scenario compared with the training phase. In addition, real environments often include new objects' categories, not included in the training data. To address this limitation, a robotic vision system should be able to learn from new training samples of known and unknown classes collected during deployment. Current techniques for incremental learning imply supervision, in the sense that a human operator has to select and annotate the new data to incorporate in the deep model. \textit{Active learning} is the challenge to overcome this limitation. To minimize user interaction, a robot should be able to select autonomously the best samples to increment the training data and specialize its deep models. In their work, \citeauthor{surveydeeplimits} also pose attention on datasets and metrics used to train and evaluate end-to-end modules. As mentioned before, a (deep) model used by an autonomous agent operates in \textit{open-set} conditions when only a small and incomplete knowledge of the real world has been used during training. Following this intuition, the authors argue that the most known and challenging datasets used in computer vision (e.g. COCO \cite{coco} or Pascal VOC \cite{pascal}) are not able to model the \textit{open-set} conditions in which a robotic vision model operates. This is because they offer a huge amount of data related only to specific object categories, making them unable to represent unknown object classes. Also the metrics (e.g. average accuracy, area under the curve, precision, recall) are not able to measure the uncertainty. These measures compute the best summary statistic over a canned dataset, so they do not generalize well the entire problem. These metrics indicate that a dataset has been solved, but it does not necessarily mean that the problem itself has been solved.  
 
 
 \section{The Role of Simulation for Robotics}
 
 As argued by \citeauthor{surveydeeplimits} in \cite{surveydeeplimits}, there is a lack of vision dataset for robotic applications, so data collection is a crucial phase to exploit a robotic vision task. Modern deep learning models are extremely data hungry: they need a large amount of data to converge and the examples must be as heterogeneous as possible. Besides the size, a well-formed dataset usable in robotic vision applications should be able to well generalize the problem it represents. The images should be captured from different positions, heights, and illumination conditions to emulate the freedom of movement that characterizes an autonomous agent. In addition, the examples should be collected from a large numbers of scenes and building types, considering indoor environments with different designs, furniture styles, and structural features. A single object radically changes its appearance based on the context to which it belongs. To collect a dataset from real world is extremely time expensive and costly. This is because the high number of robot runs to perform, the large amount of environment to consider and the physical configuration of the active agent employed. This is because simulation is widely used for this purpose. 
 
 Gibson Environment, presented in \cite{gibson}, is a real-world perception framework that can be used to acquire the necessary examples for developing robotic visual perception models. Gibson virtualizes scanned real spaces, rather than using artificially designed ones. Through Gibson, an arbitrary agent (e.g. a Turtlebot, a humanoid, or a car) can be imported and embodied respecting its physical constraints, as well as those of the real world. Gibson provides a stream of visual observation from arbitrary viewpoints as if the agent had an on-board camera. The observations include RGB images, depth data, and semantic information, to exploit the complexity of real-world environments. The main goal of Gibson is to bridge the gap between frames that come from its rendering engine and those captured directly from the real world. This is done by using a neural network rendering approach, that combines two functions: the first to make rendered frames look like the real ones, while the latter makes real images look like renderings. These two functions are trained to produce equal outputs, unifying the two domains. Gibson’s underlying database of spaces includes 572 full buildings composed of 1447 floors. Furthermore, the authors also integrate the datasets of Stanford 2D-3D \cite{stanford2d3d} and Matterport3D \cite{matterport} in Gibson for easy use. Both of these are large-scale indoor spaces dataset, that provides semantic annotations to obtain ground truth in vision tasks. In particular, the first offers 6 scanned areas from 3 real buildings of mainly educational and office use, while the former contains 90 scenes scanned from real indoor environments of any kind.
 
 Habitat is a platform for research in embodied artificial intelligence (AI) presented in \cite{habitat}. Habitat framework is composed by two modules: \textit{Habitat-Sim} and \textit{Habitat-API}. \textit{Habitat-Sim} is a flexible, high-performance 3D
 simulator with configurable agents, multiple sensors, and
 generic 3D dataset handling (with built-in support for Matterport3D \cite{matterport}, and Gibson \cite{gibson}). It is extremely fast when rendering a scene from Matterport3D, achieving thousand frames per second in a single thread and over 10,000 fps on a single GPU. \textit{Habitat-API} is a modular high-level library for end-to-end development of embodied AI algorithms concerning navigation, instruction following,
 and question answering.   
 
 In \cite{igibson}, \citeauthor{igibson} propose the Gibson Environment's evolution, called iGibson. This framework aims to unify several aspects of robot simulation, such as physics simulation for object interaction, high-quality simulated sensors data (RGB, depth, segmentation, LiDAR, and so on), integration with reinforcement learning frameworks, and realistic indoor scenes that reflect the objects' distribution of real indoor environments. This robotic simulator contains 15 fully interactive and visually realistic scenes with a total of 108 rooms. These scenes are generated  by annotating 3D reconstructions of real-world scans. The static meshes are then semantically annotated to obtain the ground truth segmentation of the various objects that populate a real scene. Then, these static environment models are converted into fully interactive scenes. This allows embodied active agents to engage in physical manipulation of articulated objects, changing the input sensor signals and the environment state. Furthermore, iGibson offers domain randomization procedures for materials (both visual appearances and dynamics properties) and object shapes applied to the object models in a scene. This facilitates the training of more robust end-to-end modules and improves their generalization properties in unseen scenes. In addition to the 15 interactive scenes, the authors support importing other datasets, such as CubiCasa5k \cite{cubicasa} and 3D-Front \cite{3dfront}. The first is a semi-automatically generated dataset of five thousand annotated floor plans of real-world homes in Finland, while the former is a large dataset of layouts designed by artists and interior designers. Both of them are converted from static scenes to fully interactive environments keeping their original structure. 
 

 