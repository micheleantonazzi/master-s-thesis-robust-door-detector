\chapter{State of the art}
\label{capitolo2}
\thispagestyle{empty}

 - Introduco il tema del riconoscimento delle porte, citando qualche articolo e delineando una storia 
 - quando arrivo ai classificatori deep, inizio a parlare di deep learning e del forte impatto che sta avendo sulla robotica
 - detto questo mi collego alla object detection e cito le milestone principali 
 - qua inizio a parlare dei problemi che deep learning e robotica implicano (survey)
 - 
 \newline\\
 In this chapter, we present  
 Mobile robots are active agents that operate interacting with real world. To successfully execute the assigned task, a mobile robot has to build an abstract model of the environment in which it operates. Considering indoor scenes, doors are crucial features that a robot can acquire to make its environment's model more informative. 
 Smart vacuum cleaners, healthcare robots or intelligent housekeepers helps people in their daily task. Usually, the tasks assigned these autonomous agents imply moving between rooms and dealing with doors.
 Doors detection can help these types of agents to safely navigate in indoor environments, by improving their reasoning abilities and navigation strategies.
 
 \section{Doors Detection using Handcrafted Features}
  In literature there are a lot of different studies concerning doors detection. One of the firsts attempts, described in \cite{sonarandivisualdoordetection}, combines visual information and sonar sensors to safely traverse doors by a B21 robot. Doors present a serious obstacle for this agent, so the goal consists into traversing opened doors using a certain angle to avoid collisions. This job is divided into two sub-task: the door detection and the door crossing. The first one is interesting for this work. The authors considers opened doors as a squared noisy
 rectangular segment in an image. To detect them, the authors use a vertical Sobel Filter to the gray scaled image. If there is a column wider than a certain threshold in the filtered image, it is considered a door. The sonar sensors are used to obtain the robot's distance from possible doors, to confirm the matches and avoid false positives. Another method for recognizing doors in unfamiliar environments is described in \cite{edgeandcornerdoorsdetector}. In this work, doors are considered as significant landmark for navigation and self-localization not only for an autonomous agent, but also for blind people. The method proposed by \citeauthor{edgeandcornerdoorsdetector} takes account of a variety of conditions including illumination
 and scale changes, deformation caused by perspective and
 occlusion, and variance of doorsâ€™ color, texture and
 appearance. At first, images are converted in gray scale mode and smoothed by Gaussian lowpass filter. Then, edges and corners are extracted from the pre-processed frames, using respectively the Canny Edge Detector (\cite{canny}) and the method described in \cite{cornerdetector}. The authors define a geometric shape
 model of doors, which is composed by two horizontal lines
 and two vertical lines between four corners. These features are then aggregated to find possible doors: only those groups that match the model are considered as true positives. The 
 
 \section{Deep Learning in Object Detection}
 As the performance of detector based on hand-crafted features became saturated, Computer Vision researchers started to use deep learning methods to perform Object Detection. The evolution of this challenging problem in Computer Vision can be found in \cite{computervisionsurvey}. Today Object Detection strongly depend on the power of deep learning. 
 After the reborn of Convolutional Neural Networks (CNNs) in 2012, \citeauthor{rcnn} propose the first deep learning paradigm to detect objects, called RCNN (Region with CNN features). RCNNs, described in \cite{rcnn}, consists of three modules. The first generates category-independent region proposals, that are sub-portion of the same image. The second module is a large Convolutional Neural Network that extracts a fixed-length feature
 vector from each region. The third module is a set of class specific linear SVMs. These classifiers predict the presence of an object within each region proposal using the relative feature vectors. Despite the great progress brought by RCNN, its drawback is obvious: the detection speed is extremely slow (about 14s per
 image with GPU). This is caused by the redundant feature computation over a large number of overlapped region proposal (over 2000 per image). To overcame this limitation, \citeauthor{sppnet} propose Spatial Pyramid Pooling Networks (SPPNet). As descried in \cite{sppnet}, this new network computes a single feature map the entire image, and then associate features to the correspondent region proposal. This method avoids the repeatedly feature extraction phase from overlapped sub-portions of the same image.  Unlike the classic CNNs, SPPNet accept as input images  of arbitrary size and generates a fixed-length representation regardless of image size/scale. In \cite{fastrcnn}, \citeauthor{fastrcnn} describes a new detector called Fast RCNN. This work unifies in the single end-to-end module the CNN responsible to extract features and the bounding box regressor, improving training and testing speed while also increasing accuracy. The next step is to generate object proposal directly with a CNN model. This technique is explained in \cite{fasterrcnn}. \citeauthor{fasterrcnn} introduce Faster RCNN: the first
 end-to-end, and the first near-realtime deep learning detector. The main contribution of this work is a Region Proposal Network (RPN) that simultaneously predicts object bounds and objectness scores at each position. Since that RPN is a convolutional network, it can be trained jointly with the entire model by sharing convolutional layers in a unique end-to-end learning framework. The training procedure alternates between fine-tuning for the region proposal task and then fine-tuning for the object detection, keeping the proposals fixed. The methods described before are also define ``two-stage detectors'', because they frame the detection as a ``coarse-to-fine'' process. In \cite{yolo}, \citeauthor{yolo} presented YOLO (You Only Look Once), introducing a new paradigm of deep models called ``one stage detectors''. This work doesn't follow the previous detection paradigm based on ``proposal detection + verification''. The neural network of YOLO is applied to the full image, performing the detection process in a single step. This method divides the image in a grid and predicts bounding boxes and class probabilities for each cell simultaneously. YOLO is also the first real-time detector: its enhanced version runs
 at 45fps while a lighter implementation reaches the 155 fps (with less detection quality).  Later, lots of improvements was made around YOLO. \citeauthor{yolov2} proposed the v2 and v3 editions \cite{yolov2, yolov3}, improving both detection speed and accuracy. Despite these great improvements, YOLO suffers from localization accuracy compared with two-stage detector, especially for small object. The second one-stage detector is called SSD (Single Shot MultiBox Detector), presented in \cite{ssd}. \citeauthor{ssd} introduce the multi reference and multi-resolution detection techniques. The main idea is to define a set of anchor boxes with different scales and aspect radios at different locations of the same image, and then predict the bounding boxes and their class using these references. Using this method, SSD significantly improves the detection performance, also for small objects. Despite their high speed, one-stage detectors have not been able to overcome two-stage detectors in accuracy. \citeauthor{focalloss} discovered the reason behind this fact: it consists into the extreme foreground-background class imbalance encountered during training. Following this intuition, the authors propose in \cite{focalloss} a new loss function, called Focal Loss, to put more focus on hard misclassified examples during training. Thanks to Focal Loss, the one-stage detectors achieve in accuracy the two-stage detectors, while maintaining very high detection speed. \citeauthor{focalloss} design a simple end-to-end module called RetinaNet to demonstrate the effectiveness of their proposed loss function.