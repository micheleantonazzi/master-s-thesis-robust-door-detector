\chapter{State of the art}
\label{capitolo2}
\thispagestyle{empty}

 Mobile robots are active agents that operate interacting with real world. To successfully execute the assigned task, a mobile robot has to build an abstract model of the environment in which it operates. Considering indoor scenes, doors are crucial features that a robot can acquire to make its environment's model more informative. 
 Smart vacuum cleaners, healthcare robots or intelligent housekeepers helps people in their daily task. Usually, the tasks assigned these autonomous agents imply moving between rooms and dealing with doors.
 Doors detection can help these types of agents to safely navigate in indoor environments, by improving their reasoning abilities and navigation strategies.
 
 \section{Doors Detection using Handcrafted Features}
  In literature there are a lot of different studies concerning doors detection. One of the firsts attempts, described in \cite{sonarandivisualdoordetection}, combines visual information and sonar sensors to safely traverse doors by a B21 robot. Doors present a serious obstacle for this agent, so the goal consists into traversing opened doors using a certain angle to avoid collisions. This activity is divided into two sub-task: the door detection and the door crossing. The first is the interesting one for this thesis. \citeauthor{sonarandivisualdoordetection} consider an opened door as a squared noisy rectangular segment in an image. To detect it, this approach applies a vertical Sobel Filter to the gray scaled image. If there is a column wider than a certain threshold in the filtered image, it is considered a door. The sonar sensors are used to obtain the robot's distance from possible doors, to confirm the matches and avoid false positives. 
  
  The method in \cite{humanoid} proposes and integrated solution to detect a door and its knob by an humanoid robot. This solution recognize the door's features with minor constraints and works sufficiently fast to performed online. \citeauthor{humanoid} argue that a door does not have a very discriminative features, and the door's appearance can vary dramatically as viewpoint changes. To solve this issue, the proposed method at first classifies a frame using other reference images. STAR Detector is used as feature extractor and the features are classified using a randomized tree classifier. At first, the reference images are parsed using the STAR Detector to find those typical characteristics of door frames. Then, a randomized tree classifier is trained using these features. At inference time, each new input image is processed by the detector to extract features and, through the decision tree, each frame is classified. If a door is detected, the robot walks toward the door and detect its knob using a segmentation technique and some general constraints (size, ratio, and height bounds).
  
  Another method for recognizing doors in unfamiliar environments is described in \cite{edgeandcornerdoorsdetector}. In this work, doors are considered as significant landmark for navigation and self-localization not only for an autonomous agent, but also for blind people. This work, proposed in \citeauthor{edgeandcornerdoorsdetector}, takes account of a variety of conditions, including differences in illumination, scale changes, deformation caused by perspective and occlusion, and variance of doorsâ€™ color, texture and appearance. At first, images are converted in gray scale and smoothed by Gaussian lowpass filter. Then, edges and corners are extracted from the pre-processed frames, using respectively the Canny Edge Detector (\cite{canny}) and the method described in \cite{cornerdetector}. The authors then define a geometric shape model of doors, which is composed by two horizontal lines and two vertical lines between four corners. These features are then aggregated to find possible doors: only those groups that match the model are considered as true positives.  
 
 \section{Deep Learning in Object Detection}
 As the performance of detector based on hand-crafted features became saturated, Computer Vision researchers started to use deep learning methods to perform Object Detection. The evolution of this challenging problem in Computer Vision can be found in \cite{computervisionsurvey}. Today Object Detection strongly depend on the power of deep learning. 
 After the reborn of Convolutional Neural Networks (CNNs) in 2012, \citeauthor{rcnn} propose the first deep learning paradigm to detect objects, called RCNN (Region with CNN features). RCNNs, described in \cite{rcnn}, consists of three modules. The first generates category-independent region proposals, that are sub-portion of the same image. The second module is a large Convolutional Neural Network that extracts a fixed-length feature
 vector from each region. The third module is a set of class specific linear SVMs. These classifiers predict the presence of an object within each region proposal using the relative feature vectors. Despite the great progress brought by RCNN, its drawback is obvious: the detection speed is extremely slow (about 14s per
 image with GPU). This is caused by the redundant feature computation over a large number of overlapped region proposal (over 2000 per image). To overcame this limitation, \citeauthor{sppnet} propose Spatial Pyramid Pooling Networks (SPPNet). As descried in \cite{sppnet}, this new network computes a single feature map the entire image, and then associate features to the correspondent region proposal. This method avoids the repeatedly feature extraction phase from overlapped sub-portions of the same image.  Unlike the classic CNNs, SPPNet accept as input images  of arbitrary size and generates a fixed-length representation regardless of image size/scale. In \cite{fastrcnn}, \citeauthor{fastrcnn} describes a new detector called Fast RCNN. This work unifies in the single end-to-end module the CNN responsible to extract features and the bounding box regressor, improving training and testing speed while also increasing accuracy. The next step is to generate object proposal directly with a CNN model. This technique is explained in \cite{fasterrcnn}. \citeauthor{fasterrcnn} introduce Faster RCNN: the first
 end-to-end, and the first near-realtime deep learning detector. The main contribution of this work is a Region Proposal Network (RPN) that simultaneously predicts object bounds and objectness scores at each position. Since that RPN is a convolutional network, it can be trained jointly with the entire model by sharing convolutional layers in a unique end-to-end learning framework. The training procedure alternates between fine-tuning for the region proposal task and then fine-tuning for the object detection, keeping the proposals fixed. The methods described before are also define ``two-stage detectors'', because they frame the detection as a ``coarse-to-fine'' process. In \cite{yolo}, \citeauthor{yolo} presented YOLO (You Only Look Once), introducing a new paradigm of deep models called ``one stage detectors''. This work doesn't follow the previous detection paradigm based on ``proposal detection + verification''. The neural network of YOLO is applied to the full image, performing the detection process in a single step. This method divides the image in a grid and predicts bounding boxes and class probabilities for each cell simultaneously. YOLO is also the first real-time detector: its enhanced version runs
 at 45fps while a lighter implementation reaches the 155 fps (with less detection quality).  Later, lots of improvements was made around YOLO. \citeauthor{yolov2} proposed the v2 and v3 editions \cite{yolov2, yolov3}, improving both detection speed and accuracy. Despite these great improvements, YOLO suffers from localization accuracy compared with two-stage detector, especially for small object. The second one-stage detector is called SSD (Single Shot MultiBox Detector), presented in \cite{ssd}. \citeauthor{ssd} introduce the multi reference and multi-resolution detection techniques. The main idea is to define a set of anchor boxes with different scales and aspect radios at different locations of the same image, and then predict the bounding boxes and their class using these references. Using this method, SSD significantly improves the detection performance, also for small objects. Despite their high speed, one-stage detectors have not been able to overcome two-stage detectors in accuracy. \citeauthor{focalloss} discovered the reason behind this fact: it consists into the extreme foreground-background class imbalance encountered during training. Following this intuition, the authors propose in \cite{focalloss} a new loss function, called Focal Loss, to put more focus on hard misclassified examples during training. Thanks to Focal Loss, the one-stage detectors achieve in accuracy the two-stage detectors, while maintaining very high detection speed. \citeauthor{focalloss} design a simple end-to-end module called RetinaNet to demonstrate the effectiveness of their proposed loss function. More recently, \citeauthor{transformer} propose a new end-to-end paradigm called Transformer \cite{transformer}. Transformers demonstrate state-of-the-art results in Natural Language Process tasks, e.g. text classification, machine translation and question answering. This new architecture intrigued researchers to study its application to computer vision problems. In \cite{surveytransformer}, \citeauthor{surveytransformer} survey the history of Transformers' implication in Computer Vision. Transformer are based on encoder-decoder architecture with self-attention mechanism. In a sequence of items, self-attention technique estimates the relevance of each item to the others, capturing the interactions between them. This enables Transformers to model long dependencies between input sequence elements and support parallel processing as compared to recurrent networks. In \cite{detr}, \citeauthor{detr} present the first end-to-end model based on Transformer to perform object detection. DETR (DEtection TRansformer) extracts features from an image using a CNN backbone and then feeds them into a classic transformer to capture their relationships. Its outputs is post-processed by a linear regressor and a multilayer perceptron, whose infer the category labels and the bounding boxes, respectively.
 
 \section{Doors Detection with Deep Learning} 
 Deep learning methods outperform traditional approaches in Computer Vision. This is because classic methods based on handcrafted features are not generalizable, meaning the features (like edges or corners) needs to be specifically aggregated for different object categories. An end-to-end module, instead, learns automatically how extract useful features that characterize an object. In particular, it extracts low level features (e.g. corner or edges) in early stages and then aggregates them in a more complex manner. It is known that these features are robust to scale, shift, rotation and exposure changes. Given their advantages, end-to-end modules have become widely used in robotics, including for exploiting doors detection tasks. The method proposed in \cite{detectdoorsfeature} is a vision-based technique for detecting doors by an autonomous agent. The main idea is to consider only colour and shape information as useful features to detect doors in an office. This approach uses two neural classifiers to recognize these specific components in an image. One is trained for detecting the top, left and the right bar of the door while the other is trained for detecting the door's corners. Then an heuristic algorithm combines these features and check if they respect a typical door structure. A door is detected if at last three of these features are found and they respect the door's geometrical constraints. Navigation is a challenging task for autonomous systems, especially in unknown and
 dynamic environment. The method described in \cite{doorsandnavigation} perform doors detection by an autonomous agents to improve its navigation strategy. This approach use a convolutional neural network to detect doors in an indoor environment. For each door in the training dataset, \citeauthor{doorsandnavigation} collect five images taken from different locations. The final goal is give to the robot the rough location of doors, then make decisions of how to move to the doors and go through them.
 Another approach, described in \cite{doorcabinet}, focuses on robustly identify doors, cabinets and their respective handles for robot grasping. \citeauthor{doorcabinet} uses a Convolutional Neural Network (based on YOLO) to detect the ROI (region of interest) of doors. Then, the proposed method obtains handle's point cloud using two different approaches. The fist one is a visual segmentation approach based on k-means color c1usterization of the ROI. The second one is a plane model extraction of the point cloud generated inside the region of interest. The ROI significantly reduces computational time and false positive rates in the previous two phases.
 
 \section{The Limits of Deep Learning for Robotics}
 
 Due the advantages and the excellent results of deep learning techniques, its application in robotics leads to specific problems and challenges that are not addressed by computer vision researchers. This is because a robot is an active agent that interacts with the real world and often operates in uncontrolled or detrimental conditions. Mistakes can lead to potentially catastrophic results and can even put human lives at risk, e.g. if the robot is a driverless car. In \cite{surveydeeplimits}, \citeauthor{surveydeeplimits} investigate the challenges of deep learning applied to robotics. They define the concept of robotic vision, which highly differs form computer vision. While the latter translates images into information, robotic vision translates images into actions, performed in the real world. For an autonomous agent, perception is only a small part of a more complicate and goal-driven system. The authors also survey the most most important learning challenges for a (deep) learning machine in a robotic vision context. The first is \textit{uncertainty estimation}. An autonomous agent have to estimate the uncertainty of its deep learning modules, considering them in the same way as other sensors. This is not trivial because deep entities return scores that are not calibrated probabilities, so their outputs are not usable in a Bayesian sensor fusion framework. In deep learning, it is assumed that a model operates in \textit{close-set} condition,  i.e. the classes encountered during deployment are known and exactly the same as during training. However, an autonomous agent operates in real world environments, that are uncontrolled and extremely different from each other. In this context, a robot will encounter instances of classes, scenarios, textures, or environmental conditions that were not covered by the training data. On this \textit{open-set} conditions, it is crucial to identify the unknowns, avoiding to recognize them as known classes. Other important challenges for robotics application using deep learning are \textit{incremental learning} and \textit{class-incremental learning}. The characteristics and appearance of objects may change a lot in the deployment scenario compared with the training phase. In addition, real environments often includes new objects' categories, not included in the training data. To address this limitation, a robotic vision system should be able to learn fro new training samples of known and unknown classes collected during deployment. Current techniques for incremental learning implies supervision, in the sense that a human operator has to select and annotate the new data to incorporate in the deep model. \textit{Active learning} is the challenge to overcome this limitation. To minimize user interaction, a robot should be able to select autonomously the best samples to increment the training data and specialize its deep models. In their work, \citeauthor{surveydeeplimits} pose attention on datasets, used to train deep models, and metrics to evaluate their performance. The authors argue that a summary statistic (e.g a metric) indicates that a dataset has been solved, it does not necessarily mean that the problem itself has been solved. The most known and challenging datasets used computer vision task (like COCO \cite{coco} or Pascal VOC \cite{pascal}) are not able to model the \textit{open-set} conditions in which a robotic vision model operates. This is because they offer a huge amount of data related only to specific object categories, making them unable to represent a unknown object classes. Also the metrics (e.g. average accuracy, area under the curve, precision, recall) are not able to measure the uncertainty. These measures compute the best summary statistic over a canned dataset, so they not generalize well the entire problem. 
 

 