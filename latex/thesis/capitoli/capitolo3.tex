\chapter{Problem Formulation}
\label{sec:chapter3}
\thispagestyle{empty}

This chapter formulates the problem we investigate in this thesis, namely door detection in autonomous mobile robots.  At first, we formally define the goal addressed by this thesis. The first one is the creation of a doors detector for mobile robots using an end-to-end technique. The second is a technique to improve the detector's performance in a certain deployment scenario, by exploiting the structural features of indoor environments. This work also tries to overcome some limitations of Deep Learning applied to robotics, such as the lack of datasets and metrics suitable for robotics applications. We proceed defining the concept of doors and the ideal deployment scenario considered by our work. Finally, we report the solutions adopted by this thesis.

\section{Goals}
\label{sec:goals}
This thesis presents a module to perform door detection by autonomous mobile robots. As discussed in the previous chapter, Researchers propose feature-based methods \cite{sonarandivisualdoordetection, humanoid, edgeandcornerdoorsdetector} and Deep Learning-based approaches \cite{detectdoorsfeature, doorsandnavigation, doorcabinet} for detecting door in indoor environments.  The module proposed in this work uses RGB images as input data, approaching the problem as an object detection task. Since Computer Vision is highly dependent on the power of Deep Learning \cite{deeplearningoverview} and the novel transformers-based architectures for object detection obtain competitive results \cite{surveytransformer}, the proposed detector is based on DETR \cite{detr}: a deep end-to-end module that take advantages of Transformers to capture the relationships between visual features vectors extracted by a CNN backbone. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN \cite{fasterrcnn} baseline on the challenging COCO object detection dataset \cite{coco}. Furthermore, DETR does not require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes (like PyTorch\footnote{The PyTorch web page: \url{https://pytorch.org/}.}). Another purpose of this thesis is to test the performance of DETR for door detection and evaluate the peculiarities of transformers in a Robotic Vision task.

The development and training of a deep module used in a Robotic Vision context have to be as much general as possible. This is because, once deployed, mobbile agents operate in unknown environments. The model's performance can degrade significantly in unfamiliar scenes since their features and structural characteristics have not been considered during the training phase. Despite this, an autonomous agent operates in a few environments during its life cycle. Intelligent agents, like service robots or smart vacuum cleaners, are deployed in a specific environment and often operate inside it for a long time. Frequently moving a robot from a scene to another is not a common scenario. Intelligent agents implement complex strategies for orienting and navigating in real environments. As robots try to understand a scene as well as possible, also artificial environments offer some feature and structural elements that help agents' perception. Researches defined the \textit{wayfinding} principle, which encompasses all of the ways with which people (and animals) orient themselves in physical space and navigate from a place to another. Following this knowledge of how humans perceive, architects and designers build artificial environments that help humans orient and navigate inside them. In \citeyear{imageofcity}, \citeauthor{imageofcity} introduced this concept in his book \citetitle{imageofcity}, where \textit{wayfinding} is defined as ``a consistent use and organization of definite sensory cues from the external environment''.  The author argues that, in the process of \textit{wayfinding}, the strategic link is the generalized mental image of the exterior physical world that is held by an individual. The coherence of the image may arise in several ways. For example, objects can be ordered or remarkable in a scene, so the user recognizes them through its previous experience and the familiarity acquired within the environment. Alternatively, an object seen for the first time is identified because it
conforms to a stereotype already constructed by the observer in other scenes. In \citeyear{wayfinding}, the \textit{wayfinding} concept was further expanded in their book entitled \citetitle{wayfinding} \cite{wayfinding}. This work describes and illustrates how people use both sings and other wayfinding cues to find their way in complex scenes, all explained into practical contexts. 
Indoor environments present a coherent design and a standardized visual aspect in order to help \textit{wayfinding} strategies used by intelligent agents. Following this intuition, there are a few types of doors  the same environment, repeated in different locations. The intuition of this thesis is to exploit the environmental features, that improve \textit{wayfinding} in indoor environments (originally realized for humans), by an autonomous agent. By considering the deployment condition of mobile robots (agents operate in the same territory for a long time) and the coherent design of indoor environments, this work proposes an approach for increasing the doors detector accuracy in unfamiliar environments. The main idea is to specialize the previously trained general doors detector for the specific environment in which the robot operates. This method, called \textit{one-shot incremental learning}, consists of collecting a sub-set of examples from the unfamiliar scene in which the robot will be deployed. Then, the detector is re-trained using these new environment-specific data. In this way, the door detection module learns how to robustly recognize the new types of doors that characterize the previously unknown deployment environment. Furthermore, this thesis investigates the amount of data required this thesis investigates the amount of data required to achieve a significant performance increase with respect to the general detector.

The power of Deep Learning is limited in vision robotics applications by some problems and challenges that researchers are still investigating \cite{surveydeeplimits}. The datasets used in Computer Vision tasks are not suitable for a robotic context. As Deep Learning is a data hungry technique, they do not contain sufficient data to generalize well the problem's domain they represent. Objects of the same category can look very different depending on the context of usage and the scene's design, so the examples must be collected from several environment types. Due to the fact that a mobile agent can autonomously explore an area, the object images to train a deep robotic detector must be captured from different viewpoints. Furthermore, the locations where data are acquired should be consistent with a possible exploration strategy followed by a mobile robot. Another limitation of the Computer Vision datasets is the lack of negative images. A negative image does not contain any object of interest. This is a shortcoming as an autonomous agent acquires and analyzes a huge amount of images but only a few of them represent positive data. In addition to datasets, also the metrics used in Computer Vision are insufficient to evaluate an end-to-end Robotic Vision module. To overcome the limitations on datasets and metrics, we propose a method for acquiring a visual dataset suitable for a Robotic Vision task. The examples are collected in virtualized heterogeneous environments scanned from real-world. The viewpoints are chosen to simulate an exploration strategy plausible for an active agent. Furthermore, this thesis proposes a novel evaluation method that considers also the negative samples.

\section{Problem Description}

Before proceeding with the detailed discussion of our work, we firstly report some definitions and concepts about the problem addressed by this thesis. In the following paragraphs, we define the ideal deployment scenario in which autonomous agents can benefit from the proposed method. Furthermore, we define the concept of door considered by this thesis, specifying the status that they can assume and the data used for their recognition.

\subsection{The Ideal Deployment Scenario}
\label{sec:deploymentscenario}
This thesis proposes an approach to improve the accuracy of a door detector module used by autonomous agents. The method proposed by this work aims to increase the doors detector's accuracy in a specific deployment scenario of mobile robots. Indoor agents are developed to operate in any environment type. This is because the final context in which a robot works is unknown and can vary a lot. Each environment type has its structural characteristics that vary a lot from each other. Furthermore, built environments of the same type can have a completely different design or visual aspect, depending on the location, the construction budget, or the occupants' culture. Since it's difficult to make assumptions on the type or the visual and structural features of the environments in which a robot is deployed, an autonomous agent must be developed in a general way. Despite this, an indoor mobile robot is typically deployed in a single or in a few spaces and does not change environment very often. This is the typical deployment scenario considered by this thesis. We consider the case in which a robot, to exploit its assigned task, navigates the same environment and finds doors to improve its reasoning abilities. The main idea is to specialize a general module that performs door detection for a single specific environment, in order to improve its performance. In the context of this thesis, we take into account only indoor environments. In particular, they are a heterogeneous set of multi-floor houses, in which each environment has a particular interior design and a different visual aspect, to better generalize the problem. Furthermore, we assume the environments are static, i.e. their structural features don't change during exploration.

\subsection{Door's Definition and Data Used} 
\label{sec:door_definition}
The second set of definitions we make is about the concept of doors and the data used by the detector to find them. The authors of the work presented in \cite{topologyurban} define the concept of \textit{portal}, which is located whenever a pair of different spaces are adjacent and no physical barriers prevent direct traversals between them.  A portal can be \textit{vertical} or \textit{horizontal}. The first type of portal connects adjacent floors in the same indoor environment (such as stairs, elevators, or ramps), while the latter unifies a tuple of locations on the same floor. The authors identify two types of horizontal portals: \textit{explicit} and \textit{implicit}. Explicit portals are connections between two adjacent spaces that include a barrier (e.g. a doorway through a wall), while implicit portals represent free connections between spaces, with no physical barrier. In the context of this thesis, we consider doors as implicit and explicit horizontal portals. In particular, explicit doors are composed of a jamb and a leaf. Otherwise, implicit doors do not have these components, typically they are only holes in walls that connect different spaces in the same environment. We consider both \textit{internal} and \textit{external} doors. The first are connections between two different spaces inside the same environment while the latter lead out of the building. The method proposed by this thesis not only detects doors but also their status. A door can be \textit{open} or \textit{closed}. A door is closed when the door leaf is completely rested on the jamb, while a door is considered open if it presents even a little hole. We do not consider any other intermediate state (e.g. semi-open) and do not define how much a door is opened. It is important to specify that only explicit doors can assume both statuses, while implicit ones are always open. The approach we describe in this thesis detects doors from RGB images. Doors are highlighted in images by drawing a rectangular bounding box around it.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\centering
		\includegraphics[width=0.24\textwidth]{images/implicitdoor1.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/implicitdoor1boxed.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/implicitdoor2.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/implicitdoor2boxed.png}
		\caption{Implicit internal open doors and the relative bounding boxes.}
	\end{subfigure}
	
	\begin{subfigure}[b]{\linewidth}
		\centering
		\includegraphics[width=0.24\textwidth]{images/explicitinternalopen1.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitinternalopen1boxed.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitinternalopen2.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitinternalopen2boxed.png}
		\caption{Explicit internal open doors and the relative bounding boxes.}
	\end{subfigure}
	
	\begin{subfigure}[b]{\linewidth}
		\centering
		\includegraphics[width=0.24\textwidth]{images/explicitinternalclosed1.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitinternalclosed1boxed.png}
		\includegraphics[width=0.24\textwidth]{images/explicitinternalclosed2.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitinternalclosed2boxed.png}
		\caption{Explicit internal closed doors and the relative bounding boxes.}	
	\end{subfigure}
	
	\begin{subfigure}[b]{\linewidth}
		\centering
		\includegraphics[width=0.24\textwidth]{images/explicitexternalclosed1.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitexternalclosed1boxed.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitexternalclosed2.png}
		\hfill
		\includegraphics[width=0.24\textwidth]{images/explicitexternalclosed2boxed.png}
		\caption{Explicit external closed doors and the relative bounding boxes.}
	\end{subfigure}
	\caption{Different types of doors with ground truth bounding boxes. Green bounding boxes represent open doorways while red rectangles denote closed doors. These images are taken form Matterport3D \cite{matterport} environments simulated by Gibson \cite{gibson}.}
\end{figure}

\section{Proposed Solution}
\label{sec:solution}
In this thesis, we face the problem of door detection in autonomous mobile robots. In the following paragraphs, we expose the proposed solutions about the three main problems this work addresses: how to build the doors detector, how to improve the module's performance, and how to compose and evaluate a visual dataset in a robotic vision context.

\subsection{Build the Doors Detector Module} At first, we reduce to problem of finding doors in indoor environments to a well-known Computer Vision task: object detection in RGB images. After a literature review about this subject, we propose a module to detect doors based on DETR (DEtection TRansformer) \cite{detr}, a deep end-to-end architecture that performs object detection exploiting the Transformers' peculiarities \cite{transformer}. DETR views object detection as a direct set prediction problem, removing the need for many hand-designed components (like non-maximum suppression or anchor generation) that explicitly encode prior knowledge about the task. Thanks to the novel Transformer's architecture, DETR extract features from a frame and then finds the pair-wire relations between them. In this way, it reasons about the entire image as context. The authors demonstrate that DETR accuracy and run-time performance are comparable to the well-established and highly-optimized Faster R-CNN baseline \cite{fasterrcnn} on the challenging COCO object detection dataset \cite{coco}. 

We choose DETR because it approaches the object detection task as a direct set prediction problem, reasoning about the entire image as a context. Furthermore, this model has a simple architecture that can be implemented in any deep learning framework that provides a common CNN backbone and a transformer architecture. This means that it can be easily customized. Despite this, DETR is an enormous network with tens of millions of parameters and it is extremely data-hungry: the authors train DETR for 300 epochs with COCO's examples randomly modified with data augmentation procedures (e.g. resize, crop, scale, etc.). Due to this fact, we do not retrain DETR from scratch but we build our doors detector starting from its pre-trained version provided by the authors. We use the standard architecture reported in the original article \cite{detr}, modifying also the object queries hyper-parameter, called $N$. It indicates the maximum number of objects contained in a single image. The author set $N = 100$, but, in the experimental phase, we argue that this value is too high with respect to our doors dataset, in which an image contains at most 4 doors instances. For this reason, we set $N = 10$. This thesis demonstrates the versatility of DETR with respect to other object detection tasks, even with smaller datasets than COCO. This is done by evaluating DETR on a well-known doors dataset, DeepDoors2 \cite{deepdoors2}, and on a visual doors dataset collected in the context of this thesis.

\subsection{Increase the Detector's Performance} Another goal of this thesis is to improve the doors detector's accuracy. To address this problem, we do not focus on the deep module that performs object detection, but we reason about its context of use. We focus on a typical deployment scenario for indoor mobile robots (as described in Sec. \ref{sec:deploymentscenario}). We argue that, often, a robot is deployed in a single environment and works inside it for a long time. Furthermore, thanks to  the wayfinding principles described in Sec. \ref{sec:goals}, different views from the same indoor scene present a coherent visual aspect. This is also true for doors: a single environment presents a few types of doors that are repeated in multiple locations. The method to increase the doors detector's accuracy exploits these intuitions. Our approach, called \textbf{one-shot incremental learning}, aims to specialize the model for working in a precise environment using the fine-tune technique. 

Fine-tuning on pre-trained ImageNet classification models \cite{verydeepimagenet, resnet} has achieved impressive results for tasks such as object detection \cite{fasterrcnn, yolo, yolov2} and is becoming the common way for solving computer vision problems. Fine-tuning is a simple and effective
approach for transfer learning: it concerns training a deep model with common large datasets (such as ImageNet \cite{imagenet} or Microsoft COCO \cite{coco}). Then, the pre-trained model is re-trained with a few new data to solve a more refined task. In this way, the network's weights are set using a source dataset with a large number of examples provide a better
model initialization than random initialization. This is useful to speed up the training, overcome small dataset size, or prevent overfitting. We use the same principle to increase the performance of the proposed doors detector. 

In our approach, called \textbf{one-shot incremental learning}, the doors detector we proposed is initially trained with a general visual doors dataset, which can be downloaded from the internet or collected in some real or simulated environments. The resulting module is a general door detector that can be deployed in any environment. Then, the general detector (built using DETR) is fine-tuned using multiple sub-sets (with different sizes) of new unseen data acquired directly in the new scene. In this way, the general door detector specializes itself using new examples for increasing its performance in a precise environment. Since the fine-tune is performed using multiple sub-sets with different sizes of new data, we also investigate the number of unseen examples necessary to obtain a significant performance improvement.

\subsection{Compose and Evaluate the Visual Dataset}
This thesis address a Robotic Vision task that completely differs from Computer Vision application. As reported in \cite{surveydeeplimits}, perception is only one part of a more complex, embodied, active, and goal-driven system in robotics.
In a simplified view, whereas Computer Vision takes images and translates them into information, Robotic Vision translates images into actions. Furthermore, an autonomous agent operates in open-set conditions. A module used by a robot can assign high-confidence scores to unknown objects or falsely recognize them as one of the known classes. 

In order to understand how the doors detector performs in different environments, this thesis proposes a method to acquire a dataset of RGB images through simulation. We collect both positive images (that contain doors to detect) and negative frames (that do not depict objects of interest) from multiple scenes. We use Gibson \cite{gibson} to simulate an agent and its visual perception in environments taken from the Matterport3D worlds dataset \cite{matterport}. A mobile robot navigates in an environment following an exploration strategy, avoiding going too close to walls, furniture, and obstacles in general. Furthermore, a robot perceives the environment from different points of view, according to its position in the scene and the height of the camera. To take this fact into account, we propose an approach to select the different locations from which to acquire the data. This method, based on the work presented by \citeauthor{repeatabilityslamarxiv} \cite{repeatabilityslamarxiv, repeatabilityslam}, computes the Voronoi Graph of the occupancy grid maps of an environment. The locations form which to acquire the RGB images are chosen by sub-sampling the Voronoi Graph with a distance value. Thank to this algorithm, the doors dataset is acquired simulating a possible exploration strategy, avoiding collecting wrong or noise images that can degrade the model accuracy.

