% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../tesi.tex

%**************************************************************
\hypertarget{Introduzione}{%
	\chapter{Introduction}\label{header-n3}}

Mobile robots are active agents that operate interacting with the real world. To successfully execute the assigned task, a mobile robot has to build an abstract model of the environment in which it operates. Considering indoor scenes, doors are crucial features that a robot can acquire to make its environment's model more informative.  Smart vacuum cleaners, healthcare robots, or intelligent housekeepers help people in their daily tasks. Usually, the tasks assigned to these autonomous agents imply moving between rooms and dealing with doors. The capabilities to detect doors, called \emph{doors detection}, can help robots to safely navigate in indoor environments, by improving their planning abilities and navigation strategies \cite{sonarandivisualdoordetection, doorsandnavigation, humanoid}. 

The first approaches for finding doors in autonomous mobile robots aim to extract hand-crafted features from different sensors data, like a sonar \cite{sonarandivisualdoordetection} or a RGB camera \cite{humanoid}. Doors represent significant landmarks for navigation and self-localization not only for an autonomous agent but also for blind people \cite{edgeandcornerdoorsdetector}. Because of this, the task of doors detection has been addressed not only by the robotics community but also by Computer Vision researchers. In this thesis, we reduce the problem of finding doors to a well-known application of Artificial Vision: object detection in RGB images. 

After the reborn of the Convolutional Neural Networks (CNNs) \cite{imagenetclassification}, the hand-crafted features methods in Computer Vision have been replaced with Deep Learning-based techniques \cite{deeplearningoverview}. A neural network automatically learns the kernels' weights to extract the features from an image, avoiding the necessity to design sophisticated feature representations. Then, following an end-to-end approach, a CNN classifies these features to produce the desired output. To better understand the modern techniques for object detection, we investigate the most relevant milestones in this discipline. In the deep learning era, object detection can be grouped
into two genres: ``two-stage detection'' and ``one-stage detection', where the former frames the detection as a ``coarse-to-fine'' process while the latter frames it as to ``complete in one step''. The two-stage detectors (such as RCNN \cite{rcnn}, Fast-RCNN \cite{fastrcnn}, and Faster-RCNN \cite{fasterrcnn})  generate region proposals from an image, then extract a fixed-length feature vector using a CNN for each sub-portion of the image, and finally classify each feature vector to find the presence of an object. Otherwise, the ``one-stage'' detectors (e.g. YOLO \cite{yolo} and its evolutions \cite{yolov2, yolov3}) apply a single convolution neural network to the
full image. These networks divide the image into regions
and predict bounding boxes and object probabilities for each
region simultaneously, reducing significantly the inference time. More recently, the Computer Vision community has started to use the new paradigm of Transformers \cite{transformer}. This architecture, initially developed for natural language process tasks, models the relationships between a sequence of items (a list of words in a NLP application or a feature representation in a Computer Vision task). The new approaches for object detection, like DETR \cite{detr}, are able to reason considering the entire image as a unique context.

The goal of this thesis is to propose a doors detector for autonomous mobile robots. We approach the problem of detecting doors as a visual object detection task translated in a robotic context. In literature, there are many approaches that aim to detect doors in autonomous agents using RGB data \cite{doorsandnavigation, detectdoorsfeature} and/or depth information \cite{doorcabinet}. The doors detector we propose is based on DETR \cite{detr}, a deep end-to-end architecture that performs object detection by combining the power of ResNet \cite{resnet}, a CNN backbone that produces a compact representation from an image, and a Transformer \cite{transformer} that models the complex relationships between the extracted features.

We also propose an approach, called \textbf{one-shot incremental learning}, that aims to increase the accuracy of the proposed detector. We consider a typical deployment scenario in which an autonomous agent is set up in a specific environment and operates inside it for a long time (maybe for its entire life cycle). Then, we reason about this scenario in  conjunction with the \textit{wayfinding} principle \cite{wayfinding, imageofcity}, which encompasses all of the ways with which people (and animals) orient themselves in physical space and navigate from a place to another. Following this knowledge of how humans perceive, architects and designers build artificial environments that help humans orient and navigate inside them. A way to address these requirements is to maintain a coherent end clear visual aspect in an indoor scene. Following this principle, we argue that the doors inside the same environment are similar to each other. In other words, a single environment presents a few types of doors repeated in multiple locations.  The \textbf{one-shot incremental learning} technique we propose aims to specialize a general doors detector with a few new examples captured directly from a new environment in which the robot can be deployed. In this way, we produce an optimized version of the general detector which obtains better performance in a specific environment. This thesis also investigates the necessary amount of new examples to obtain a significant accuracy improvement.

Since we propose doors detector based on an end-to-end model, this thesis deals with some challenges of Deep Learning applied to robotics. The work reported in \cite{surveydeeplimits} highlights the difference between Computer Vision and Robotic Vision. In particular, the first is only a sub-portion of the latter: perception is only one part of a more complex, embodied, active, and goal-driven system. In particular, the authors argue that Computer Vision translates images into information, while Robotic Vision translates images into actions in the real world.

The challenges we address regards the lack of visual datasets and evaluation metrics suitable for robotics contexts. First of all, as reported in \cite{surveydeeplimits}, an autonomous agent operates in \textit{open-set} conditions, so a deep learning model can encounter different instances of classes, scenarios, or textures not covered by the training data. Furthermore, a robot acquires a large number of images during its activity but only a few of them contains objects of interest for a certain detection task. In addition, a robot perceives the real world following some constraints due to its physical characteristics and to the exploration strategy it follows. The well-known datasets used in Computer Vision \cite{coco, imagenet, pascal}, in addition to not containing doors, do not correctly model the typical uncertainty in which a robot operates. Furthermore, the images of these datasets do not depict the objects in unreachable or unlikely points of view for an autonomous mobile robot. The existing metrics for evaluating an object detection task \cite{pascal, generalizediou, coco} do not consider negative images (without objects of interest). To overcome these limitations, this thesis proposes a method to collect a visual dataset in batch from multiple environments through simulations. We also develop a tool to extract the positions from which to acquire the examples. Following the works published in \cite{repeatabilityslamarxiv, repeatabilityslam}, this tool processes a 2D occupancy grid map and computes the Voronoi graph to get a pool of plausible locations for a real mobile robot. Since the dataset collected in this thesis is composed also of negative images, we propose a new evaluation metric (based on those of Pascal VOC challenge \cite{pascal}) to evaluate the bounding boxes in negative images. 

This thesis is organized as follows:

\begin{itemize}
	\item \textbf{Chapter \ref{sec:chapter2}:} in this chapter, we report the state-of-the-art regarding the challenge of finding doors in autonomous mobile robots. We start by analyzing feature-based methods to detect doors for mobile robots. Then, we survey the history of Computer Vision since the advent of deep learning, reviewing also some approaches to detect doors based on this paradigm. Finally, this chapter analyzes the limitation of Deep Learning applied to robotics and describes the importance of simulation in a robotic context.
	
	\item \textbf{Chapter \ref{sec:chapter3}:} here, we report the formulation of the problem addressed by this thesis. At first, we discuss the importance of doors detection in mobile robots, we define the goals and the assumptions of this work. In the end, we describe the proposed solution for reaching the goals previously described.
	
	\item \textbf{Chapter \ref{sec:chapter4}:} this chapter reports the details of the system we develop to address the objectives of this thesis. We described the architecture and the functionalities of DETR \cite{detr}, the end-to-end module used for building the doors detector. We proceed by analyzing the simulation technologies we use to collect the dataset and how we modify them to enable a fast data collection procedure. This chapter reports a description of the dataset, explaining how it is labeled and the characteristics of the framework we develop to manage it. Finally, we describe the algorithm to extract the positions from which acquire the dataset and the metric we use to evaluate the doors detector's performance.
	
	\item \textbf{Chapter \ref{sec:chapter5}:} in this chapter, we report the experimental evaluation about the doors detector and the technique we propose to increase its accuracy in a specific environment. At first, we describe how  the dataset is acquired. Then, we provide a test of DETR trained with a well-known door dataset (DeepDoors2 \cite{deepdoors2}) to verify if this model performs well in a doors detection task and to understand how to train it with a smaller dataset than COCO \cite{coco}. Finally, we report the experimental results of the \textbf{one-shot incremental learning} approach.
	
	\item \textbf{Chapter \ref{sec:chapter6}:} here, we summarize our work by analyzing the obtained results and giving some suggestions for future researches and improvements.
\end{itemize}
