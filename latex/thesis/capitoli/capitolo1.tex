% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../tesi.tex

%**************************************************************
\hypertarget{Introduzione}{%
	\chapter{Introduction}\label{header-n3}}

Mobile robots are agents that operate by physically interacting with the real world. To successfully execute the assigned task, a mobile robot has to build an abstract model of the environment in which it operates that represents all features of interest for its activities. Considering indoor scenes, the location of doors are crucial features that a robot should have in its environment's model.  Smart vacuum cleaners, healthcare robots, or intelligent housekeepers help people in their daily tasks. Usually, the tasks assigned to these autonomous agents imply moving between rooms and dealing with passages. The capabilities to detect doors, called \emph{door detection}, can help robots to safely navigate in indoor environments, by improving their planning abilities and navigation strategies \cite{sonarandivisualdoordetection, doorsandnavigation, humanoid}. 

The first approaches for finding doors in autonomous mobile robots aim to extract hand-crafted features from different sensors data, like a sonar \cite{sonarandivisualdoordetection} or a RGB camera \cite{humanoid}. Doors represent significant landmarks for navigation and self-localization not only for an autonomous agent but also for blind people \cite{edgeandcornerdoorsdetector}. Because of this, the task of door detection has been addressed not only by the robotics community but also by Computer Vision researchers. 

The goal of this thesis is to propose a doors detector for autonomous mobile robots. We approach the problem of detecting doors as a visual object detection task deployed in a robotic context. In literature, there are many approaches that aim to detect doors in autonomous agents using RGB data \cite{doorsandnavigation, detectdoorsfeature} and/or depth information \cite{doorcabinet}. The doors detector we propose is based on DETR \cite{detr}, a deep end-to-end architecture that performs object detection by combining the power of ResNet \cite{resnet}, a CNN backbone that produces a compact representation from an image, and a Transformer \cite{transformer} that models the complex relationships between the extracted features.

We also propose an approach, called \textbf{one-shot incremental learning}, that aims to increase the accuracy of the proposed detector. We consider a typical deployment scenario in which an autonomous agent is set up in a specific environment and operates inside it for a long time (eventually for its entire life cycle). Then, we reason about this scenario in  conjunction with the \textit{wayfinding} principle \cite{wayfinding, imageofcity}, which encompasses all of the ways with which people orient themselves in physical space and navigate from a place to another. Following this knowledge of how humans perceive, architects and designers build artificial environments that help humans to orient and navigate inside them. A way to address these requirements is to maintain a coherent end clear visual aspect in an indoor scene. Following this principle, we argue that the doors inside the same environment are similar to each other. In other words, a single environment presents a few types of doors repeated in multiple locations. The idea we follow is that a machine learning model for finding doors used by an autonomous agent is initially trained on a general dataset to be suitable for any context of use. The module's performance can be variable according to the visual aspect of the scene in which the robot operates. The \textbf{one-shot incremental learning} technique we propose aims to specialize a general doors detector with a few new examples captured directly from a new environment in which the robot will be deployed. In this way, the robot learns new  repeated features of interest of the specific environment in which it operates. 
 In other words, the technique proposed in this thesis produces an optimized version of a general doors detector which obtains better performance in a specific environment. This thesis also investigates the necessary amount of new examples to obtain a significant accuracy improvement.

Since we propose doors detector based on an end-to-end model, this thesis deals with some challenges of Deep Learning applied to robotics. The work reported in \cite{surveydeeplimits} highlights the difference between Computer Vision and Robotic Vision. In particular, the first is only a sub-portion of the latter: perception is only one part of a more complex, embodied, active, and goal-driven system. In particular, the authors argue that Computer Vision translates images into information, while Robotic Vision translates images into actions in the real world.

The challenges we address regard the lack of visual datasets and evaluation metrics suitable for our robotic context. First of all, as reported in \cite{surveydeeplimits}, an autonomous agent operates in \textit{open-set} conditions, so a deep learning model can encounter different instances of classes, scenarios, or textures not covered by the training data. Furthermore, a robot acquires a large number of images during its activity but only a few of them contain objects of interest for a certain detection task. In addition, a robot perceives the real world following some constraints due to its physical characteristics and to the exploration strategy it follows. The well-known datasets used in Computer Vision \cite{coco, imagenet, pascal} do not correctly model the typical uncertainty in which a robot operates. First of all, they do not contain negative images (without objects of interest). Then, the images of these datasets do not depict the objects in unreachable or unlikely points of view for an autonomous mobile robot. Another issue regards the metrics (e.g. those reported in \cite{pascal, generalizediou, coco}) for evaluating the performance of a detector that do not consider negative images (without objects of interest). To overcome these limitations, this thesis proposes a method to build a visual dataset in batch from multiple environments through simulations. We also develop a tool to extract the positions from which to acquire the examples. Following the works published in \cite{repeatabilityslamarxiv, repeatabilityslam}, this tool processes a 2D occupancy grid map and computes the Voronoi graph to get a pool of plausible locations for a real mobile robot. In addition, we propose a new evaluation metric (based on those of Pascal VOC challenge \cite{pascal}) to evaluate the bounding boxes in negative images. 

This thesis is organized as follows:

\begin{itemize}
	\item \textbf{Chapter \ref{sec:chapter2}:} in this chapter, we report the state-of-the-art regarding the challenge of finding doors in autonomous mobile robots. We start by reporting some examples of the importance of door detection for autonomous mobile robots. Then, we analyze feature-based methods to detect doors for mobile robots. Then, we overview some of the concepts of Machine Learning, Deep Learning, and Computer Vision that are of interest for this work, by also focusing on some approaches to detect doors based on these paradigms. Finally, this chapter reports some limitations of Deep Learning applied to robotics and describes the importance of simulation in a robotic context.
	
	\item \textbf{Chapter \ref{sec:chapter3}:} in this chapter, we report the formulation of the problem addressed by this thesis. At first, we define the goals and the assumptions of this work. In the end, we describe the proposed solution for reaching the goals previously described.
	
	\item \textbf{Chapter \ref{sec:chapter4}:} this chapter reports the details of the system we develop to address the objectives of this thesis. We describe the architecture and the functionalities of DETR \cite{detr}, the end-to-end module used for building the doors detector. We proceed by analyzing the simulation technologies we use to collect the dataset and how we modify them to enable a fast data collection procedure. This chapter reports a description of the dataset, explaining how it is labeled and the characteristics of the framework we develop to manage it. Finally, we describe the algorithm to extract the positions from which we acquire the dataset and the metric we use to evaluate the doors detector's performance.
	
	\item \textbf{Chapter \ref{sec:chapter5}:} in this chapter, we report the experimental evaluation about the doors detector and the technique we propose to increase its accuracy in a specific environment. At first, we describe how  the dataset is acquired. Then, we provide a test of DETR trained with a well-known door dataset (DeepDoors2 \cite{deepdoors2}) to verify if this model performs well in a door detection task and to understand how to train it with a smaller dataset than COCO \cite{coco}. Finally, we report the experimental results of the \textbf{one-shot incremental learning} approach.
	
	\item \textbf{Chapter \ref{sec:chapter6}:} in this chapter, we summarize our work by analyzing the obtained results and give some suggestions for future research and improvements.
\end{itemize}
