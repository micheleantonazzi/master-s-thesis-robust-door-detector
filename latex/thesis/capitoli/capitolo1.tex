% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../tesi.tex

%**************************************************************
\hypertarget{Introduzione}{%
	\chapter{Introduction}\label{header-n3}}

Mobile robots are active agents that operate interacting with the real world. To successfully execute the assigned task, a mobile robot has to build an abstract model of the environment in which it operates. Considering indoor scenes, doors are crucial features that a robot can acquire to make its environment's model more informative.  Smart vacuum cleaners, healthcare robots or intelligent housekeepers help people in their daily tasks. Usually, the tasks assigned to these autonomous agents imply moving between rooms and dealing with doors. The capabilities to detect doors, called \emph{doors detection}, can help robots to safely navigate in indoor environments, by improving their planning abilities and navigation strategies \cite{sonarandivisualdoordetection, doorsandnavigation, humanoid}. 

The firsts approaches for finding doors in autonomous mobile robots aim to extract hand-crafted features from different sensors data, like a sonar \cite{sonarandivisualdoordetection} or a RGB camera \cite{humanoid}. Doors represent significant landmarks for navigation and self-localization not only for an autonomous agent but also for blind people \cite{edgeandcornerdoorsdetector}. Because of this, the task of doors detection has been addressed not only by the robotics community but also by Computer Vision researches. In this thesis, we reduce the problem of finding doors to a well-known application of Artificial Vision: object detection in RGB images. 

After the reborn of the Convolutional Neural Networks (CNNs) \cite{imagenetclassification}, the hand-crafted features methods in Copmputer Vision have been replaced with Deep Learning based techniques \cite{deeplearningoverview}. A neural network automatically learns the kernels weights to extract the features from an image, avoiding the necessity do design sophisticated feature representations. Then, following an end-to-end approach, a CNN classify these features to produce the desired output. To better understande the modern techniques for object detection, we investigate the most relevant milestones in this a discipline. In deep learning era, object detection can be grouped
into two genres: ``two-stage detection'' and ``one-stage detection', where the former frames the detection as a ``coarse-to-fine'' process while the later frames it as to ``complete in one step''. The two-stage detectors (such as RCNN \cite{rcnn}, Fast-RCNN \cite{fastrcnn}, and Faster-RCNN \cite{fasterrcnn})  generate category-independent region proposals from an image, then extract a fixed-length feature vector using a CNN for each sub-portion of the image, and finally classify each feature vector to find the presence of an object. Otherwise, the ``one stage'' detectors (e.g. YOLO \cite{yolo} and its evolutions \cite{yolov2, yolov3}) apply a single neural network to the
full image. These networks divide the image into regions
and predict bounding boxes and probabilities for each
region simultaneously, reducing significantly the inference time. More recently, the Computer Vision community have started to use the new paradigm of Transformers \cite{transformer}. This architecture, initially developed for natural language process tasks, models the relationship between a sequence of item (a list of words in a NLP application or a feature representation in Computer Vision task). The new approaches for object detection, like DETR \cite{detr}, are able to reason considering the entire image as a unique context.

The goal of this thesis is to propose a doors detector for autonomous mobile robots. We approach the problem of detecting doors as a visual object detection in task translated in a robotic context. In literature, there are many approaches that aim to detect doors in autonomous agents using RGB data \cite{doorsandnavigation, detectdoorsfeature} and/or depth information \cite{doorcabinet}. The doors detector we propose is based on DETR \cite{detr}, a deep end-to-end architecture that performs object detection by combining the power of ResNet \cite{resnet}, a CNN backbone that produces a compact representation from an image, and a Transformer \cite{transformer} that models the complex relationships between the extracted features.

We also propose an approach, called \textbf{one-shot incremental learning}, that aims to increase the proposed detector. We consider a typical deployment scenario in which an autonomous agent is set up in a specific environment and operates inside it for a long time (maybe for its entire life cycle). Then, we reason about this scenario in  conjunction with the \textit{wayfinding} principle \cite{wayfinding, imageofcity}, which encompasses all of the ways with which people (and animals) orient themselves in physical space and navigate from a place to another. Following this knowledge of how humans perceive, architects and designers build artificial environments that help humans orient and navigate inside them. A way is to address this requirements is to maintain a coherent end clear visual aspect of an indoor scene. Following this principle, we argue that the doors inside the same environment are similar to each other. In other words, a single environment presents a few types of doors repeated in multiple locations.  The \textbf{one-shot incremental learning} technique we propose aims to specialize a general doors detector with a few new examples captured directly from a new environment in which the robot can be deployed. In this way, we produce an optimized version of the general detector which obtains better performance in a specific environment. This thesis also investigates the necessary amount of new example to obtain a significant accuracy improvement.

Since we propose doors detector based on an end-to-end model, this thesis deals with some challenges of Deep Learning applied to robotics. The work reported in \cite{surveydeeplimits} highlights the difference between Computer Vision and Robotic Vision. In particular, the first is only a sub-portion of the latter: perception is only one part of a more complex, embodied, active, and goal-driven system. In particular, the authors argue that Computer Vision translates images into information, while Robotic Vision translates images into actions in the real world.

The challenges we address regards the lack of visual datasets and evaluation metrics suitable for robotics contexts. First of all, as reported in \cite{surveydeeplimits}, an autonomous agent operates in \textit{open-set} conditions and a deep learning model can encounter different instances of classes, scenarios, ot texture outside the training data. Furthermore, a robot acquire a large amount of images during its activity but only a few of them contains object of interest for a certain detection task. In addition, a robot perceive the real world following some constraints due to its physical characteristics and to the exploration strategy it follows. The well-known datasets used in Computer Vision \cite{coco, imagenet, pascal}, in addition to not containing ports, do not correctly model the typical uncertainty typical in the context of robotics. Furthermore, it do not depict the objects in unreachable or unlikely points of view for an autonomous mobile robot. In addition, the existing metrics for evaluating an object detection task \cite{pascal, generalizediou, coco} do not consider negative images (without objects of interest). 
